{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees Tutorial - Student Template\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Decision Trees are a popular machine learning algorithm that can be used for both classification and regression tasks. They work by splitting the data into subsets based on feature values, creating a tree-like structure of decisions.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **Root Node**: The topmost node that represents the entire dataset\n",
    "2. **Internal Nodes**: Nodes that represent decisions based on feature values\n",
    "3. **Leaf Nodes**: Terminal nodes that represent the final output/prediction\n",
    "4. **Branches**: Paths from one node to another\n",
    "\n",
    "### Advantages of Decision Trees:\n",
    "- Easy to understand and interpret\n",
    "- Can handle both numerical and categorical data\n",
    "- Requires little data preprocessing\n",
    "- Non-parametric method (no assumptions about data distribution)\n",
    "\n",
    "### Disadvantages of Decision Trees:\n",
    "- Prone to overfitting, especially with deep trees\n",
    "- Can be unstable (small changes in data can lead to different trees)\n",
    "- Biased toward features with more levels in categorical data\n",
    "\n",
    "In this tutorial, we'll explore how to implement decision trees using scikit-learn with a real-world dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "# TODO: Import numpy, pandas, matplotlib, seaborn, and sklearn modules\n",
    "# Hint: You'll need datasets, train_test_split, DecisionTreeRegressor, DecisionTreeClassifier,\n",
    "#       mean_squared_error, r2_score, accuracy_score, classification_report, and plot_tree\n",
    "\n",
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: California Housing\n",
    "\n",
    "We'll use the California Housing dataset, which contains information about houses in California districts. This dataset has 20,640 samples and 8 features, making it a good example of a \"big dataset\" for machine learning.\n",
    "\n",
    "### Features:\n",
    "1. **MedInc**: Median income in block group\n",
    "2. **HouseAge**: Median house age in block group\n",
    "3. **AveRooms**: Average number of rooms per household\n",
    "4. **AveBedrms**: Average number of bedrooms per household\n",
    "5. **Population**: Block group population\n",
    "6. **AveOccup**: Average number of household members\n",
    "7. **Latitude**: Block group latitude\n",
    "8. **Longitude**: Block group longitude\n",
    "\n",
    "### Target:\n",
    "**MedHouseVal**: Median house value for California districts (in hundreds of thousands)\n",
    "\n",
    "Let's load the dataset and explore its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the California Housing dataset\n",
    "# TODO: Use fetch_california_housing() to load the dataset\n",
    "# TODO: Create a DataFrame with the feature data and add the target variable\n",
    "# Hint: Use pd.DataFrame() and add the target as a new column\n",
    "\n",
    "# Your code here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic statistics of the dataset\n",
    "# TODO: Use the describe() method to show statistics\n",
    "\n",
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "\n",
    "Let's visualize the distribution of our target variable and some key features to better understand the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the target variable (Median House Value)\n",
    "# TODO: Create a histogram of the MedHouseVal column\n",
    "# Hint: Use plt.hist() and add appropriate labels and title\n",
    "\n",
    "# Your code here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlation matrix to understand relationships between features\n",
    "# TODO: Create a heatmap of the correlation matrix\n",
    "# Hint: Use sns.heatmap() with the correlation matrix from df.corr()\n",
    "\n",
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data for Decision Tree\n",
    "\n",
    "Before training our decision tree, we need to split our data into training and testing sets. This allows us to evaluate how well our model generalizes to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and target variable (y)\n",
    "# TODO: Create X (features) by dropping the target column\n",
    "# TODO: Create y (target) with just the target column\n",
    "# TODO: Split the data using train_test_split with test_size=0.2 and random_state=42\n",
    "\n",
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Decision Tree Regressor\n",
    "\n",
    "Now we'll train a Decision Tree Regressor to predict house values. We'll use scikit-learn's `DecisionTreeRegressor` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Decision Tree Regressor model\n",
    "# TODO: Create a DecisionTreeRegressor with max_depth=10 and random_state=42\n",
    "# TODO: Fit the model on the training data\n",
    "\n",
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "\n",
    "Let's use our trained model to make predictions on the test set and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "# TODO: Use the predict method to make predictions on X_test\n",
    "# TODO: Calculate MSE, RMSE, and R² score using the appropriate sklearn functions\n",
    "\n",
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Predictions vs Actual Values\n",
    "\n",
    "Let's visualize how well our model's predictions match the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot of actual vs predicted values\n",
    "# TODO: Create a scatter plot with y_test on x-axis and y_pred on y-axis\n",
    "# TODO: Add a diagonal reference line\n",
    "# TODO: Add appropriate labels and title\n",
    "\n",
    "# Your code here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residuals\n",
    "# TODO: Calculate residuals (y_test - y_pred)\n",
    "# TODO: Create a scatter plot of predicted values vs residuals\n",
    "# TODO: Add a horizontal line at y=0\n",
    "# TODO: Add appropriate labels and title\n",
    "\n",
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "One of the advantages of decision trees is that they provide insight into which features are most important for making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances from the decision tree\n",
    "# TODO: Extract feature importances from the trained model\n",
    "# TODO: Create a DataFrame with features and their importances\n",
    "# TODO: Sort by importance (descending)\n",
    "# TODO: Print the feature importances\n",
    "# TODO: Create a bar plot of feature importances\n",
    "\n",
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Decision Tree\n",
    "\n",
    "Let's visualize a simplified version of our decision tree to understand how it makes decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simpler decision tree for visualization (limiting depth to 3)\n",
    "# TODO: Create a DecisionTreeRegressor with max_depth=3 and random_state=42\n",
    "# TODO: Fit the model on training data\n",
    "# TODO: Use plot_tree to visualize the tree\n",
    "# Hint: Use feature_names parameter to label the features\n",
    "\n",
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "Decision trees have several hyperparameters that can be tuned to improve performance. Let's experiment with different values for `max_depth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different max_depth values\n",
    "# TODO: Create a range of max_depth values from 1 to 20\n",
    "# TODO: For each depth, train a model and calculate train/test R² scores\n",
    "# TODO: Store the scores in lists\n",
    "# TODO: Plot the results\n",
    "# TODO: Find and print the best depth and score\n",
    "\n",
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with Other Models\n",
    "\n",
    "Let's compare our decision tree with a simple baseline model (mean predictor) to see how much improvement we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model: always predict the mean\n",
    "# TODO: Create predictions that are all equal to the mean of y_train\n",
    "# TODO: Calculate baseline MSE, RMSE, and R²\n",
    "# TODO: Compare with your decision tree performance\n",
    "# TODO: Calculate the improvement percentage\n",
    "\n",
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Example\n",
    "\n",
    "Decision trees can also be used for classification tasks. Let's create a binary classification problem by categorizing houses as \"expensive\" or \"affordable\" based on their median value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a binary classification target\n",
    "# TODO: Calculate the median of MedHouseVal\n",
    "# TODO: Create a binary target where 1 = expensive (above median) and 0 = affordable (below or equal to median)\n",
    "# TODO: Add the new columns to the DataFrame\n",
    "# TODO: Print statistics about the classification\n",
    "\n",
    "# Your code here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for classification\n",
    "# TODO: Create X_class (features) and y_class (binary target)\n",
    "# TODO: Split the data using train_test_split\n",
    "# TODO: Create and train a DecisionTreeClassifier\n",
    "# TODO: Make predictions\n",
    "# TODO: Calculate and print accuracy\n",
    "# TODO: Print classification report\n",
    "\n",
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we've explored how to use decision trees for both regression and classification tasks:\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Decision trees are versatile**: They can be used for both regression (predicting continuous values) and classification (predicting categories)\n",
    "\n",
    "2. **Data preprocessing is important**: We split our data into training and testing sets to evaluate model performance properly\n",
    "\n",
    "3. **Hyperparameter tuning matters**: We experimented with different `max_depth` values to find the optimal balance between underfitting and overfitting\n",
    "\n",
    "4. **Model evaluation is crucial**: We used multiple metrics (MSE, RMSE, R² for regression; accuracy for classification) to assess our models\n",
    "\n",
    "5. **Interpretability is a strength**: Decision trees provide feature importances and can be visualized to understand how decisions are made\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Try other hyperparameters like `min_samples_split`, `min_samples_leaf`, and `max_features`\n",
    "2. Experiment with ensemble methods like Random Forest or Gradient Boosting\n",
    "3. Apply decision trees to other datasets\n",
    "4. Handle overfitting with techniques like pruning or setting constraints\n",
    "\n",
    "Decision trees are a powerful and interpretable machine learning method that serves as a foundation for more complex algorithms. Understanding them is crucial for any machine learning practitioner!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
