{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCCdBj6x6hKu"
      },
      "outputs": [],
      "source": [
        "# Install open-source packages only\n",
        "!pip install -q sentence-transformers chromadb pypdf langchain langchain-community faiss-cpu transformers torch numpy pandas\n",
        "\n",
        "# Verify installations\n",
        "import subprocess\n",
        "result = subprocess.run(['pip', 'list'], capture_output=True, text=True)\n",
        "print(\"Installed packages:\")\n",
        "for line in result.stdout.split('\\n'):\n",
        "    if any(pkg in line for pkg in ['sentence-transformers', 'chromadb', 'langchain', 'faiss']):\n",
        "        print(f\"  {line.strip()}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import all required libraries\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any\n",
        "from datetime import datetime\n",
        "\n",
        "# PDF processing\n",
        "from pypdf import PdfReader\n",
        "\n",
        "# Vector stores and embeddings (all open-source)\n",
        "import chromadb\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Document handling\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Progress tracking\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "print(\"All libraries imported successfully!\")"
      ],
      "metadata": {
        "id": "bwtJDufL6nVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sample research paper content (simulating a real PDF)\n",
        "sample_research_content = \"\"\"\n",
        "Title: Deep Learning for Natural Language Processing: A Comprehensive Survey\n",
        "\n",
        "Abstract: This paper presents a comprehensive survey of deep learning techniques applied to natural language processing tasks. We examine the evolution from traditional statistical methods to modern neural architectures, focusing on transformer-based models and their applications.\n",
        "\n",
        "1. Introduction\n",
        "Natural Language Processing (NLP) has undergone a revolutionary transformation with the advent of deep learning. Traditional rule-based systems have given way to neural networks that learn patterns from vast amounts of text data.\n",
        "\n",
        "2. Background: Traditional NLP Methods\n",
        "Before deep learning, NLP relied heavily on:\n",
        "- Bag-of-words models\n",
        "- N-gram language models\n",
        "- Hidden Markov Models (HMMs)\n",
        "- Conditional Random Fields (CRFs)\n",
        "\n",
        "3. Neural Network Foundations\n",
        "Deep learning in NLP builds upon several key neural architectures:\n",
        "- Recurrent Neural Networks (RNNs) for sequential data\n",
        "- Long Short-Term Memory (LSTM) networks for long dependencies\n",
        "- Convolutional Neural Networks (CNNs) for local patterns\n",
        "\n",
        "4. Transformer Architecture\n",
        "The transformer model, introduced in \"Attention is All You Need,\" revolutionized NLP through:\n",
        "- Self-attention mechanisms\n",
        "- Parallel processing capabilities\n",
        "- Scalability to large datasets\n",
        "\n",
        "5. Large Language Models\n",
        "Modern LLMs like BERT, GPT, and T5 demonstrate:\n",
        "- Few-shot learning capabilities\n",
        "- Transfer learning effectiveness\n",
        "- Emergent behaviors at scale\n",
        "\n",
        "6. Applications and Future Directions\n",
        "Current applications include machine translation, question answering, and text generation. Future research focuses on efficiency, interpretability, and reducing computational requirements.\n",
        "\"\"\"\n",
        "\n",
        "# Save as a sample PDF file\n",
        "with open('sample_research_paper.txt', 'w') as f:\n",
        "    f.write(sample_research_content)\n",
        "\n",
        "print(\"Sample research paper created!\")\n",
        "print(\"File: sample_research_paper.txt\")\n",
        "print(f\"Size: {len(sample_research_content)} characters\")"
      ],
      "metadata": {
        "id": "00y3jV_U6qkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PDFProcessor:\n",
        "    \"\"\"Handles PDF reading and text processing\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=500,  # Each chunk ~500 characters\n",
        "            chunk_overlap=50,  # Overlap to maintain context\n",
        "            separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \"]  # Smart splitting\n",
        "        )\n",
        "\n",
        "    def load_pdf_content(self, file_path: str) -> str:\n",
        "        \"\"\"Load content from PDF or text file\"\"\"\n",
        "        try:\n",
        "            if file_path.endswith('.pdf'):\n",
        "                reader = PdfReader(file_path)\n",
        "                text = \"\"\n",
        "                for page in reader.pages:\n",
        "                    text += page.extract_text() + \"\\n\"\n",
        "            else:\n",
        "                with open(file_path, 'r') as f:\n",
        "                    text = f.read()\n",
        "\n",
        "            print(f\"âœ… Loaded {len(text)} characters from {file_path}\")\n",
        "            return text\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading file: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def create_documents(self, text: str, source_name: str) -> List[Document]:\n",
        "        \"\"\"Convert text into LangChain documents\"\"\"\n",
        "        # Split text into chunks\n",
        "        chunks = self.text_splitter.split_text(text)\n",
        "\n",
        "        # Create documents with metadata\n",
        "        documents = []\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            doc = Document(\n",
        "                page_content=chunk,\n",
        "                metadata={\n",
        "                    \"source\": source_name,\n",
        "                    \"chunk_id\": i,\n",
        "                    \"chunk_size\": len(chunk),\n",
        "                    \"timestamp\": datetime.now().isoformat()\n",
        "                }\n",
        "            )\n",
        "            documents.append(doc)\n",
        "\n",
        "        print(f\"Created {len(documents)} document chunks\")\n",
        "        return documents\n",
        "\n",
        "# Initialize processor\n",
        "processor = PDFProcessor()\n",
        "\n",
        "text=processor.load_pdf_content(\"/content/CNN.pdf\")\n",
        "\n",
        "# Process our sample research paper\n",
        "documents = processor.create_documents(sample_research_content, \"research_paper_survey\")\n",
        "\n",
        "# Show first few chunks\n",
        "print(\"\\nðŸ“‹ First 3 chunks:\")\n",
        "for i, doc in enumerate(documents[:3]):\n",
        "    print(f\"\\n--- Chunk {i+1} ---\")\n",
        "    print(f\"Content: {doc.page_content[:200]}...\")\n",
        "    print(f\"Size: {doc.metadata['chunk_size']} characters\")"
      ],
      "metadata": {
        "id": "QAziPLEU60sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingManager:\n",
        "    \"\"\"Handles text embeddings using open-source models\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # This is a small, fast, open-source model\n",
        "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        self.embedding_dim = 384 # Size of embedding vectors\n",
        "\n",
        "        print(f\"Loaded embedding model: all-MiniLM-L6-v2\")\n",
        "        print(f\"Embedding dimension: {self.embedding_dim}\")\n",
        "\n",
        "    def create_embeddings(self, texts: List[str]) -> np.ndarray:\n",
        "        \"\"\"Convert texts to embedding vectors\"\"\"\n",
        "        print(\"Creating embeddings ...\")\n",
        "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
        "        print(f\"Created {len(embeddings)} embeddings\")\n",
        "        return embeddings\n",
        "\n",
        "# Initialize embedding manager\n",
        "embed_manager = EmbeddingManager()\n",
        "\n",
        "# Test with a simple example\n",
        "test_texts = [\"Machine learning is amazing\", \"Deep learning uses neural networks\", \"AI transforms the world\"]\n",
        "test_query = \"neural networks\"\n",
        "embeddings = embed_manager.create_embeddings(test_texts)\n",
        "print(f\"Created {len(embeddings)} embeddings for test texts\")"
      ],
      "metadata": {
        "id": "KUCxLNdmKTsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VectorStoreManager:\n",
        "    \"\"\"Manages the in-memory vector database\"\"\"\n",
        "\n",
        "    def __init__(self, embedding_manager: EmbeddingManager):\n",
        "        self.embedding_manager = embedding_manager\n",
        "\n",
        "        # Create in-memory Chroma client\n",
        "        self.client = chromadb.Client()\n",
        "\n",
        "        # Delete the collection if it exists\n",
        "        try:\n",
        "            self.client.delete_collection(name=\"research_papers\")\n",
        "            print(\"Deleted existing collection 'research_papers'\")\n",
        "        except:\n",
        "            pass # Ignore if collection doesn't exist\n",
        "\n",
        "        # Create or get collection\n",
        "        self.collection = self.client.create_collection(\n",
        "            name=\"research_papers\",\n",
        "            metadata={\"description\": \"Academic paper chunks\"}\n",
        "        )\n",
        "\n",
        "    def add_documents(self, documents: List[Document]):\n",
        "        \"\"\"Add documents to the vector store\"\"\"\n",
        "        print(\"ðŸ”„ Adding documents to vector store...\")\n",
        "        # Prepare data\n",
        "        texts = [doc.page_content for doc in documents]\n",
        "        metadatas = [doc.metadata for doc in documents]\n",
        "        ids = [f\"doc_{i}\" for i in range(len(documents))]\n",
        "        # Create embeddings\n",
        "        embeddings = self.embedding_manager.create_embeddings(texts)\n",
        "        # Add to collection\n",
        "        self.collection.add(\n",
        "            embeddings=embeddings.tolist(),\n",
        "            documents=texts,\n",
        "            metadatas=metadatas,\n",
        "            ids=ids\n",
        "        )\n",
        "        print(f\"Added {len(documents)} documents to vector store\")\n",
        "\n",
        "    def search(self, query: str, n_results: int = 3) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Search for similar documents\"\"\"\n",
        "        # Create query embedding\n",
        "        query_embedding = self.embedding_manager.create_embeddings([query])[0]\n",
        "        results = self.collection.query(\n",
        "            query_embeddings=[query_embedding.tolist()],\n",
        "            n_results=n_results\n",
        "        )\n",
        "        # Format results\n",
        "        formatted_results = []\n",
        "        for i in range(len(results['documents'][0])):\n",
        "            formatted_results.append({\n",
        "                'content': results['documents'][0][i],\n",
        "                'metadata': results['metadatas'][0][i],\n",
        "                'distance': results['distances'][0][i]\n",
        "            })\n",
        "        return formatted_results\n",
        "\n",
        "# Initialize vector store\n",
        "vector_store = VectorStoreManager(embed_manager)\n",
        "vector_store.add_documents(documents)\n",
        "\n",
        "# Test search\n",
        "search_results = vector_store.search(\"What is deep learning?\", n_results=2)\n",
        "print(\"\\nSearch Results\")\n",
        "\n",
        "for i,result in enumerate(search_results,1):\n",
        "    print(f\"\\n--- Result {i+1} ---\")\n",
        "    print(f\"Content: {result['content'][:200]}\")\n",
        "    print(f\"Distance: {result['distance']:.3f}\")"
      ],
      "metadata": {
        "id": "3pVk4P3sTFD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "class OpenSourceLLM:\n",
        "    \"\"\"Open-source language model for Q&A\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Using Flan-T5-small for educational purposes\n",
        "        model_name = \"google/flan-t5-small\"\n",
        "        print(\"Loading open-source language model...\")\n",
        "        self.qa_pipeline = pipeline(\n",
        "            \"text2text-generation\",\n",
        "            model=model_name,\n",
        "            tokenizer=model_name,\n",
        "        )\n",
        "        print(f\"Loaded model: {model_name}\")\n",
        "\n",
        "    def generate_answer(self, question: str, context: str) -> str:\n",
        "        \"\"\"Generate answer using context\"\"\"\n",
        "        # Create prompt\n",
        "        prompt = f\"\"\"Answer the question based on the context provided.\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "\n",
        "        # Generate response\n",
        "        response = self.qa_pipeline(prompt, max_length=200, do_sample=True)\n",
        "        return response[0]['generated_text']\n",
        "\n",
        "# Initialize LLM\n",
        "llm = OpenSourceLLM()\n",
        "\n",
        "# Test the model\n",
        "test_context = \"Deep learning is a subset of machine learning that uses neural networks with multiple layers.\"\n",
        "test_question = \"What is deep learning?\"\n",
        "test_answer = llm.generate_answer(test_question, test_context)\n",
        "print(f\"\\nðŸ” Test Answer: {test_answer}\")"
      ],
      "metadata": {
        "id": "dNruI4i9cINC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResearchAssistant:\n",
        "    \"\"\"Complete RAG system for research papers\"\"\"\n",
        "\n",
        "    def __init__(self, vector_store: VectorStoreManager, llm: OpenSourceLLM):\n",
        "        self.vector_store = vector_store\n",
        "        self.llm = llm\n",
        "        self.conversation_history = []\n",
        "\n",
        "    def ask_question(self, question: str, n_contexts: int = 3) -> Dict[str, Any]:\n",
        "        \"\"\"Ask a question about the research paper\"\"\"\n",
        "        print(f\"Processing: {question}\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Step 1: Find relevant contexts\n",
        "        relevant_docs = self.vector_store.search(question, n_results=n_contexts)\n",
        "\n",
        "        # Step 2: Combine contexts\n",
        "        combined_context = \"\\n\\n\".join([doc['content'] for doc in relevant_docs])\n",
        "\n",
        "        # Step 3: Generate answer\n",
        "        answer = self.llm.generate_answer(question, combined_context)\n",
        "\n",
        "        processing_time = time.time() - start_time\n",
        "\n",
        "        # Store conversation\n",
        "        result = {\n",
        "            \"question\": question,\n",
        "            \"answer\": answer,\n",
        "            \"contexts_used\": len(relevant_docs),\n",
        "            \"processing_time\": round(processing_time, 2),\n",
        "            \"sources\": [doc['metadata'] for doc in relevant_docs]\n",
        "        }\n",
        "\n",
        "        self.conversation_history.append(result)\n",
        "\n",
        "        return result\n",
        "\n",
        "# Initialize the research assistant\n",
        "assistant = ResearchAssistant(vector_store, llm)\n",
        "\n",
        "print(\"Research Assistant is ready!\")\n",
        "print(\"You can now ask questions about the research paper.\")"
      ],
      "metadata": {
        "id": "MaVef11XyC2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Educational questions about the research paper\n",
        "educational_questions = [\n",
        "    \"What is the difference between traditional NLP and deep learning NLP?\",\n",
        "    \"Can you explain what a transformer is in simple terms?\",\n",
        "    \"What are the main applications of deep learning in NLP?\",\n",
        "    \"How do neural networks help with language understanding?\",\n",
        "    \"What comes before deep learning in NLP history?\"\n",
        "]\n",
        "\n",
        "print(\"ðŸŽ“ Educational Questions & Answers:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for question in educational_questions:\n",
        "    print(f\"\\nQuestion: {question}\")\n",
        "    result = assistant.ask_question(question)\n",
        "    print(f\"Answer: {result['answer']}\")\n",
        "    print(f\"Processing time: {result['processing_time']}s\")\n",
        "    print(f\"Sources used: {result['contexts_used']} chunks\")"
      ],
      "metadata": {
        "id": "0I6ux55B3tmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ðŸŽ‰ Congratulations! You've built a complete GenAI system!\")\n",
        "print(\"\\nWhat you learned:\")\n",
        "print(\"How to process PDF documents into searchable chunks\")\n",
        "print(\"Using open-source embedding models (no API keys!)\")\n",
        "print(\"Building in-memory vector databases with ChromaDB\")\n",
        "print(\"Creating Q&A systems with open-source language models\")\n",
        "print(\"Adding educational features for better learning\")\n",
        "\n",
        "print(\"\\n Next steps to explore:\")\n",
        "print(\"1. Try with your own PDF research papers\")\n",
        "print(\"2. Experiment with different embedding models\")\n",
        "print(\"3. Add conversation memory for follow-up questions\")\n",
        "print(\"4. Create a web interface using Streamlit\")\n",
        "print(\"5. Try larger open-source models like Llama-2\")\n",
        "\n",
        "# Save conversation history for review\n",
        "with open('learning_session.json', 'w') as f:\n",
        "    json.dump(assistant.conversation_history, f, indent=2)\n",
        "\n",
        "print(\"\\nConversation history saved to 'learning_session.json'\")"
      ],
      "metadata": {
        "id": "iyzgkVns3w9L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}