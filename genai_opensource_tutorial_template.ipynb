{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmB0ATAxoWyV"
      },
      "source": [
        "# GenAI with Open-Source Models: Complete Tutorial\n",
        "## Building RAG Systems with Local Models & PDF Processing\n",
        "\n",
        "**ðŸ“š What you'll learn:**\n",
        "- How to use **completely open-source** models (no API keys needed!)\n",
        "- Process **real PDF documents** with practical use cases\n",
        "- Build **in-memory vector stores** for fast prototyping\n",
        "- Create **educational explanations** for each step\n",
        "\n",
        "** Use Case:** Build a **Research Paper Assistant** that helps students understand academic papers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0-NhP6koWyh"
      },
      "source": [
        "## Step 1: Environment Setup (No API Keys!)\n",
        "\n",
        "**What we're doing:** Installing only open-source packages that work completely offline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yJA54FhoWyi"
      },
      "outputs": [],
      "source": [
        "# Install open-source packages only\n",
        "!pip install -q sentence-transformers chromadb pypdf langchain langchain-community faiss-cpu transformers torch numpy pandas\n",
        "\n",
        "# Verify installations\n",
        "import subprocess\n",
        "result = subprocess.run(['pip', 'list'], capture_output=True, text=True)\n",
        "print(\"Installed packages:\")\n",
        "for line in result.stdout.split('\\n'):\n",
        "    if any(pkg in line for pkg in ['sentence-transformers', 'chromadb', 'langchain', 'faiss']):\n",
        "        print(f\"  {line.strip()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybn7wp6xoWyk"
      },
      "outputs": [],
      "source": [
        "# Import all required libraries\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any\n",
        "from datetime import datetime\n",
        "\n",
        "# PDF processing\n",
        "from pypdf import PdfReader\n",
        "\n",
        "# Vector stores and embeddings (all open-source)\n",
        "import chromadb\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Document handling\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Progress tracking\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "print(\"All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAgNd_jLoWyl"
      },
      "source": [
        "## Step 2: Understanding Our Use Case\n",
        "\n",
        "**Scenario:** You're a student who needs to understand research papers quickly. We'll build a system that:\n",
        "1. Reads PDF research papers\n",
        "2. Creates a searchable knowledge base\n",
        "3. Answers questions about the paper content\n",
        "4. Explains complex concepts in simple terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYDnXOw2oWyl"
      },
      "outputs": [],
      "source": [
        "# Create sample research paper content (simulating a real PDF)\n",
        "sample_research_content = \"\"\"\n",
        "Title: Deep Learning for Natural Language Processing: A Comprehensive Survey\n",
        "\n",
        "Abstract: This paper presents a comprehensive survey of deep learning techniques applied to natural language processing tasks. We examine the evolution from traditional statistical methods to modern neural architectures, focusing on transformer-based models and their applications.\n",
        "\n",
        "1. Introduction\n",
        "Natural Language Processing (NLP) has undergone a revolutionary transformation with the advent of deep learning. Traditional rule-based systems have given way to neural networks that learn patterns from vast amounts of text data.\n",
        "\n",
        "2. Background: Traditional NLP Methods\n",
        "Before deep learning, NLP relied heavily on:\n",
        "- Bag-of-words models\n",
        "- N-gram language models\n",
        "- Hidden Markov Models (HMMs)\n",
        "- Conditional Random Fields (CRFs)\n",
        "\n",
        "3. Neural Network Foundations\n",
        "Deep learning in NLP builds upon several key neural architectures:\n",
        "- Recurrent Neural Networks (RNNs) for sequential data\n",
        "- Long Short-Term Memory (LSTM) networks for long dependencies\n",
        "- Convolutional Neural Networks (CNNs) for local patterns\n",
        "\n",
        "4. Transformer Architecture\n",
        "The transformer model, introduced in \"Attention is All You Need,\" revolutionized NLP through:\n",
        "- Self-attention mechanisms\n",
        "- Parallel processing capabilities\n",
        "- Scalability to large datasets\n",
        "\n",
        "5. Large Language Models\n",
        "Modern LLMs like BERT, GPT, and T5 demonstrate:\n",
        "- Few-shot learning capabilities\n",
        "- Transfer learning effectiveness\n",
        "- Emergent behaviors at scale\n",
        "\n",
        "6. Applications and Future Directions\n",
        "Current applications include machine translation, question answering, and text generation. Future research focuses on efficiency, interpretability, and reducing computational requirements.\n",
        "\"\"\"\n",
        "\n",
        "# Save as a sample PDF file\n",
        "with open('sample_research_paper.txt', 'w') as f:\n",
        "    f.write(sample_research_content)\n",
        "\n",
        "print(\"Sample research paper created!\")\n",
        "print(\"File: sample_research_paper.txt\")\n",
        "print(f\"Size: {len(sample_research_content)} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpeQkrWdoWym"
      },
      "source": [
        "## Step 3: PDF Processing Explained\n",
        "\n",
        "**What happens here:**\n",
        "- We read the PDF file\n",
        "- Split it into manageable chunks (like paragraphs)\n",
        "- Add metadata so we know where each piece came from"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxFHOBdToWym"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfLNjerRoWyn"
      },
      "source": [
        "## Step 4: Open-Source Embeddings Explained\n",
        "\n",
        "**What are embeddings?**\n",
        "- Think of them as \"smart fingerprints\" for text\n",
        "- Similar texts have similar fingerprints\n",
        "- We use the **all-MiniLM-L6-v2** model (completely free and offline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Ioop-N3oWyo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81ieo1cWoWyp"
      },
      "source": [
        "## Step 5: In-Memory Vector Store (ChromaDB)\n",
        "\n",
        "**What is ChromaDB?**\n",
        "- A lightweight, in-memory database for vectors\n",
        "- Perfect for prototyping and learning\n",
        "- No setup required - works immediately!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O48X_iSdoWyp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ze-LnetToWyq"
      },
      "source": [
        "## Step 6: Open-Source Language Model\n",
        "\n",
        "**What we're using:**\n",
        "- **Flan-T5** - Google's open-source model\n",
        "- **Completely free** - runs on your machine\n",
        "- **Good for educational purposes** - explains concepts clearly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcipYHnNoWyq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfrOOy0hoWyr"
      },
      "source": [
        "## Step 7: Complete RAG Pipeline\n",
        "\n",
        "**Putting it all together:**\n",
        "- **R**etrieval: Find relevant chunks from PDF\n",
        "- **A**ugmentation: Add context to the question\n",
        "- **G**eneration: Create answer using open-source model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvWCsk48oWyr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JzaQMIxoWyr"
      },
      "source": [
        "## Step 8: Interactive Learning Session\n",
        "\n",
        "**Let's test our system with educational questions!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFdwSNuIoWyr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yApJxVbmoWys"
      },
      "source": [
        "## Final Summary & Next Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mALqgxPeoWys"
      },
      "outputs": [],
      "source": [
        "print(\"ðŸŽ‰ Congratulations! You've built a complete GenAI system!\")\n",
        "print(\"\\nWhat you learned:\")\n",
        "print(\"How to process PDF documents into searchable chunks\")\n",
        "print(\"Using open-source embedding models (no API keys!)\")\n",
        "print(\"Building in-memory vector databases with ChromaDB\")\n",
        "print(\"Creating Q&A systems with open-source language models\")\n",
        "print(\"Adding educational features for better learning\")\n",
        "\n",
        "print(\"\\n Next steps to explore:\")\n",
        "print(\"1. Try with your own PDF research papers\")\n",
        "print(\"2. Experiment with different embedding models\")\n",
        "print(\"3. Add conversation memory for follow-up questions\")\n",
        "print(\"4. Create a web interface using Streamlit\")\n",
        "print(\"5. Try larger open-source models like Llama-2\")\n",
        "\n",
        "# Save conversation history for review\n",
        "with open('learning_session.json', 'w') as f:\n",
        "    json.dump(assistant.conversation_history, f, indent=2)\n",
        "\n",
        "print(\"\\nConversation history saved to 'learning_session.json'\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}